"""
Inference Pipeline for RadAssist Pro.

Provides high-level inference orchestration including:
- Preprocessing
- Model inference
- Post-processing
- Report generation

This module coordinates the complete analysis workflow.
"""

import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
import json

from .medgemma_wrapper import (
    MedGemmaModel,
    InferenceResult,
    VolumetricResult,
    LongitudinalResult,
    MedGemmaConfig
)

logger = logging.getLogger(__name__)


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class AnalysisRequest:
    """Request for AI analysis."""
    image_path: Union[str, Path]
    study_type: str = "chest_xray"
    clinical_context: Optional[str] = None
    patient_id: Optional[str] = None
    study_id: Optional[str] = None
    modality: str = "CR"
    urgent: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalysisReport:
    """Complete analysis report."""
    # Header
    report_id: str
    patient_id: str
    study_id: str
    study_type: str
    study_date: str
    generated_at: str

    # Body
    findings: List[str]
    impression: str
    confidence: float
    abnormalities: List[str]

    # Metadata
    model_version: str
    processing_time_ms: float
    is_urgent: bool

    # Disclaimer
    disclaimer: str = (
        "⚠️ AI-GENERATED REPORT - FOR RESEARCH PURPOSES ONLY\n"
        "This report was generated by an AI system (RadAssist Pro).\n"
        "NOT FOR CLINICAL USE. Not FDA-cleared.\n"
        "All findings must be verified by a qualified radiologist."
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "header": {
                "report_id": self.report_id,
                "patient_id": self.patient_id,
                "study_id": self.study_id,
                "study_type": self.study_type,
                "study_date": self.study_date,
                "generated_at": self.generated_at
            },
            "body": {
                "findings": self.findings,
                "impression": self.impression,
                "confidence": self.confidence,
                "abnormalities": self.abnormalities
            },
            "metadata": {
                "model_version": self.model_version,
                "processing_time_ms": self.processing_time_ms,
                "is_urgent": self.is_urgent
            },
            "disclaimer": self.disclaimer
        }

    def to_json(self, indent: int = 2) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict(), indent=indent)

    def to_text(self) -> str:
        """Convert to human-readable text report."""
        lines = [
            "=" * 60,
            "RADASSIST PRO - AI RADIOLOGY ANALYSIS REPORT",
            "=" * 60,
            "",
            f"Report ID: {self.report_id}",
            f"Patient ID: {self.patient_id}",
            f"Study ID: {self.study_id}",
            f"Study Type: {self.study_type}",
            f"Study Date: {self.study_date}",
            f"Generated: {self.generated_at}",
            "",
            "-" * 60,
            "FINDINGS:",
            "-" * 60,
        ]

        for i, finding in enumerate(self.findings, 1):
            lines.append(f"  {i}. {finding}")

        lines.extend([
            "",
            "-" * 60,
            "IMPRESSION:",
            "-" * 60,
            f"  {self.impression}",
            "",
            f"Confidence: {self.confidence:.1%}",
        ])

        if self.abnormalities:
            lines.extend([
                "",
                "Abnormalities Detected:",
                "  " + ", ".join(self.abnormalities)
            ])

        if self.is_urgent:
            lines.extend([
                "",
                "⚠️ URGENT: This study contains potentially urgent findings.",
            ])

        lines.extend([
            "",
            "=" * 60,
            self.disclaimer,
            "=" * 60
        ])

        return "\n".join(lines)


# =============================================================================
# Inference Pipeline
# =============================================================================

class InferencePipeline:
    """
    Orchestrates the complete inference workflow.

    Example:
        pipeline = InferencePipeline()

        request = AnalysisRequest(
            image_path="chest_xray.png",
            study_type="chest_xray",
            patient_id="SYNTH_001"
        )

        report = pipeline.analyze(request)
        print(report.to_text())
    """

    def __init__(
        self,
        model: Optional[MedGemmaModel] = None,
        config: Optional[MedGemmaConfig] = None
    ):
        """
        Initialize inference pipeline.

        Args:
            model: Pre-loaded MedGemma model (or will create new)
            config: Model configuration
        """
        self.model = model
        self.config = config or MedGemmaConfig()
        self._report_counter = 0

    def ensure_model_loaded(self) -> bool:
        """Ensure model is loaded."""
        if self.model is None:
            self.model = MedGemmaModel(self.config)

        if not self.model.is_loaded():
            return self.model.load()

        return True

    def analyze(self, request: AnalysisRequest) -> AnalysisReport:
        """
        Perform complete analysis workflow.

        Args:
            request: Analysis request

        Returns:
            Complete analysis report
        """
        logger.info(f"Starting analysis for {request.study_type}: {request.image_path}")

        # Ensure model is ready
        self.ensure_model_loaded()

        # Run inference based on study type
        if request.study_type in ["ct_volume", "mri_volume", "3d"]:
            result = self._analyze_3d(request)
        else:
            result = self._analyze_2d(request)

        # Generate report
        report = self._create_report(request, result)

        logger.info(f"Analysis complete. Confidence: {report.confidence:.1%}")

        return report

    def analyze_batch(
        self,
        requests: List[AnalysisRequest]
    ) -> List[AnalysisReport]:
        """
        Analyze multiple studies.

        Args:
            requests: List of analysis requests

        Returns:
            List of analysis reports
        """
        reports = []
        for request in requests:
            try:
                report = self.analyze(request)
                reports.append(report)
            except Exception as e:
                logger.error(f"Batch analysis failed for {request.image_path}: {e}")

        return reports

    def compare_studies(
        self,
        study_paths: List[Union[str, Path]],
        patient_id: str,
        clinical_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Compare multiple studies longitudinally.

        Args:
            study_paths: Paths to studies (oldest first)
            patient_id: Patient identifier
            clinical_context: Optional clinical context

        Returns:
            Comparison report dictionary
        """
        self.ensure_model_loaded()

        result = self.model.compare_longitudinal(study_paths, clinical_context)

        self._report_counter += 1

        return {
            "report_id": f"COMP_{self._report_counter:06d}",
            "patient_id": patient_id,
            "comparison_type": "longitudinal",
            "timepoints": len(study_paths),
            "summary": result.comparison_summary,
            "changes": result.changes_detected,
            "confidence": result.confidence,
            "recommendation": result.recommendation,
            "generated_at": datetime.utcnow().isoformat(),
            "disclaimer": AnalysisReport.disclaimer
        }

    def _analyze_2d(self, request: AnalysisRequest) -> InferenceResult:
        """Perform 2D image analysis."""
        return self.model.infer(
            image_path=request.image_path,
            clinical_context=request.clinical_context,
            study_type=request.study_type
        )

    def _analyze_3d(self, request: AnalysisRequest) -> VolumetricResult:
        """Perform 3D volume analysis."""
        return self.model.infer_3d(
            volume_path=request.image_path,
            clinical_context=request.clinical_context
        )

    def _create_report(
        self,
        request: AnalysisRequest,
        result: Union[InferenceResult, VolumetricResult]
    ) -> AnalysisReport:
        """Create analysis report from inference result."""
        self._report_counter += 1

        # Determine if urgent based on findings
        urgent_keywords = [
            "pneumothorax", "tension", "aortic dissection",
            "pulmonary embolism", "acute", "emergent"
        ]
        is_urgent = any(
            keyword in str(result.findings).lower()
            for keyword in urgent_keywords
        ) or request.urgent

        return AnalysisReport(
            report_id=f"RAD_{self._report_counter:06d}",
            patient_id=request.patient_id or "ANONYMOUS",
            study_id=request.study_id or f"STUDY_{self._report_counter:06d}",
            study_type=request.study_type,
            study_date=request.metadata.get("study_date", datetime.utcnow().strftime("%Y-%m-%d")),
            generated_at=datetime.utcnow().isoformat(),
            findings=result.findings,
            impression=result.impression or "See detailed findings",
            confidence=result.confidence,
            abnormalities=result.abnormalities if hasattr(result, 'abnormalities') else [],
            model_version=result.model_version,
            processing_time_ms=result.processing_time_ms,
            is_urgent=is_urgent
        )


# =============================================================================
# Convenience Functions
# =============================================================================

def quick_analyze(
    image_path: Union[str, Path],
    study_type: str = "chest_xray"
) -> AnalysisReport:
    """
    Quick analysis without full pipeline setup.

    Convenience function for simple use cases.

    Args:
        image_path: Path to medical image
        study_type: Type of study

    Returns:
        Analysis report
    """
    pipeline = InferencePipeline()

    request = AnalysisRequest(
        image_path=image_path,
        study_type=study_type
    )

    return pipeline.analyze(request)


def analyze_with_context(
    image_path: Union[str, Path],
    clinical_context: str,
    study_type: str = "chest_xray"
) -> AnalysisReport:
    """
    Analyze image with clinical context.

    Args:
        image_path: Path to medical image
        clinical_context: Clinical history/context
        study_type: Type of study

    Returns:
        Analysis report
    """
    pipeline = InferencePipeline()

    request = AnalysisRequest(
        image_path=image_path,
        study_type=study_type,
        clinical_context=clinical_context
    )

    return pipeline.analyze(request)
